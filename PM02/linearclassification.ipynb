{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Linear Classification\n",
    "\n",
    "revision: dcfbda7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# PREAMBLE\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.optimize import check_grad\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_context(\"notebook\", font_scale=1.1)\n",
    "sns.set_style(\"ticks\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Classifying Rock Types\n",
    "\n",
    "Our next real world example will be from geology.\n",
    "More precisely from *rock physics*, which studies the relation between physical and elastic properties of rocks and is the basis of quantitative seismic interpretation.\n",
    "In this scenario, we got the following data set with the task to distinguish between shale and sand rock types.\n",
    "As sample of the data looks as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load all data\n",
    "df = pd.read_csv('rocktypes.csv', sep=' ')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In order to feed the data to an algorithm, we have to convert the string 'type' to a numerical label ($\\{-1, +1\\}$ in this case).\n",
    "Create a new column 'label' which contains the value 1 if the type is 'sand' and -1 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.scatterplot(ax=ax, x='impedance', y='velocity', hue='type', data=df, palette=dict(sand='orange', shale='gray'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Insert your code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# assert that the label column is correct\n",
    "np.testing.assert_array_equal(np.sum(df['label'] == +1), 258)\n",
    "np.testing.assert_array_equal(np.sum(df['label'] == -1), 173)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Exploring the data\n",
    "\n",
    "Before starting on any task, it is often useful to understand the data by visualizing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "sns.scatterplot(ax=ax, x='impedance', y='velocity', hue='type', data=df, palette=dict(sand='orange', shale='gray'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Feature Scaling\n",
    "\n",
    "1. Scale the columns `df['impedance']` and `df['velocity']` such that they have zero mean and unit variance.\n",
    "Do not overwrite the columns, but create new ones as `df['impedance normalized']` and `df['velocity normalized']`.\n",
    "2. Add a bias column `df['bias']`.\n",
    "3. Plot the normalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Insert your code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df['impedance normalized'] = (df['impedance'] - df['impedance'].mean()) / df['impedance'].std()\n",
    "df['velocity normalized'] = (df['velocity'] - df['velocity'].mean()) / df['velocity'].std()\n",
    "df['bias'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "sns.scatterplot(ax=ax, x='impedance normalized', y='velocity normalized', hue='type', data=df, palette=dict(sand='orange', shale='gray'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# export pandas dataframe to numpy array\n",
    "X = df[['bias', 'impedance normalized', 'velocity normalized', ]].values\n",
    "y = df['label'].values\n",
    "(n, d) = X.shape\n",
    "\n",
    "print(f\"Dimensions X: {X.shape}, y: {y.shape}\")\n",
    "np.testing.assert_array_equal(X.shape, (431, 3))\n",
    "np.testing.assert_array_equal(y.shape, (431,))\n",
    "np.testing.assert_array_almost_equal(X[0, :], [1., -0.4344, -0.0095], decimal=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Open losses.py and implement the function logistic_loss\n",
    "from mlis.classification.losses import logistic_loss\n",
    "\n",
    "np.testing.assert_array_almost_equal(logistic_loss(0.4, 1), 0.513, decimal=3)\n",
    "np.testing.assert_array_almost_equal(logistic_loss(0.4, -1), 0.913, decimal=3)\n",
    "np.testing.assert_array_almost_equal(logistic_loss(np.array([-2.1, 0.2, 1.3]), np.array([1, -1, 1])),\n",
    "                                     [2.2155, 0.7981, 0.241], decimal=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Open linearclassification.py and implement the function h (can be copied from linear regression)\n",
    "from mlis.classification.linearclassification import h\n",
    "\n",
    "w = np.array([2, 3, -1])\n",
    "np.testing.assert_array_almost_equal(h(w, X)[:3], [0.71, 0.71, 0.72], decimal=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Open logisticregression.py and implement the function J\n",
    "from mlis.classification.logisticregression import J\n",
    "\n",
    "w = np.array([2, 3, -1])\n",
    "np.testing.assert_array_almost_equal(J(w, X, y), 3.0116, decimal=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Implementing gradient descent\n",
    "\n",
    "Now that we have the hypothesis and the cost function we can implement the gradient descent routine. First, write a function $\\nabla J(w)$ which returns the gradient of the cost function. The gradient is just a vector with all the partial derivatives\n",
    "$$\n",
    "\\nabla J(w) = \\bigg(\\frac{\\partial J(w)}{\\partial w_1} , \\dotsc, \\frac{\\partial J(w)}{\\partial w_n} \\bigg)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Open logisticregression.py and implement the function dJ\n",
    "from mlis.classification.logisticregression import dJ\n",
    "\n",
    "w = np.array([2, 3, -1])\n",
    "assert dJ(w, X, y).shape == (3,)  # we expect a vector of size (3,)\n",
    "np.testing.assert_almost_equal(check_grad(J, dJ, w, X, y), 0.0, decimal=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Open linearclassification.py and implement the function gradientDescent (can be copyied from linear regression)\n",
    "from mlis.classification.linearclassification import gradientDescent\n",
    "\n",
    "w0 = [1, 1, 1]  # initial weights\n",
    "eta = 0.05  # learning rate Î·\n",
    "iters = 1000  # number of iterations\n",
    "\n",
    "ws = gradientDescent(dJ, X, y, w0, eta, iters)\n",
    "# the last entry contains the final parameters found by gradient decent\n",
    "print(f\"best weights w: {ws[-1]}\")\n",
    "\n",
    "# testing the correctness of gradientDescent\n",
    "np.testing.assert_array_almost_equal(ws[-1], [0.998, -0.9523, 3.6905], decimal=3)  # test optimal weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Open losses.py and implement zero_one_loss\n",
    "from mlis.classification.losses import zero_one_loss\n",
    "\n",
    "np.testing.assert_array_almost_equal(zero_one_loss(0.4, 1), 0, decimal=3)\n",
    "np.testing.assert_array_almost_equal(zero_one_loss(0.4, -1), 1, decimal=3)\n",
    "np.testing.assert_array_almost_equal(zero_one_loss(np.array([-2.1, 0.2, 1.3, 1.3]), np.array([1, -1, 1, -1])),\n",
    "                                     [1, 1, 0, 1], decimal=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Verifying Gradient Descent\n",
    "\n",
    "A good way to verify that gradient descent is working correctly is to look at the values $J(w)$ and check that it is decreasing with each step.\n",
    "Compare the following plot with figure 3.6 on page 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "costs = [J(w, X, y) for w in ws]\n",
    "errorrate = [np.mean(zero_one_loss(h(w, X), y)) for w in ws]\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "ax.plot(np.arange(len(costs)), costs, 'k', label='J(w)')\n",
    "ax.plot(np.arange(len(errorrate)), errorrate, label='error rate')\n",
    "ax.set_xlabel('Iterations')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We use the final parameters to plot the linear fit. Compare this to figure 3.7 on page 19."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_fit(h, w, levels=[0]):\n",
    "    x_range = df['impedance normalized'].min(), df['impedance normalized'].max()\n",
    "    y_range = df['velocity normalized'].min(), df['velocity normalized'].max()\n",
    "    grid = 100\n",
    "    xx, yy = np.meshgrid(np.linspace(x_range[0], x_range[1], grid), np.linspace(y_range[0], y_range[1], grid))\n",
    "    XY = np.array([np.ones((grid * grid,)), np.ravel(xx), np.ravel(yy)]).T\n",
    "    P = h(w, XY).reshape(grid, grid)\n",
    "\n",
    "    _, ax = plt.subplots()\n",
    "    sns.scatterplot(ax=ax, x='impedance normalized', y='velocity normalized', hue='type', data=df, palette=dict(sand='orange', shale='gray'))\n",
    "    cn = ax.contour(xx, yy, P, colors='k', levels=levels)\n",
    "    ax.clabel(cn, inline=1, fontsize=10)\n",
    "\n",
    "\n",
    "plot_fit(h, ws[-1])\n",
    "plot_fit(h, ws[-1], levels=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Probabilistic Prediction\n",
    "\n",
    "Implement the logistic function\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1+\\exp(-z)}\n",
    "$$\n",
    "in order to make probabilistic prediction.\n",
    "Compare the result to figure 3.8 on page 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Open losses.py and implement the logistic function\n",
    "from mlis.classification.losses import logistic\n",
    "\n",
    "np.testing.assert_array_almost_equal(logistic(np.array([0.1, 8, -2])), [0.525, 1., 0.119], decimal=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_fit(lambda w, X: logistic(h(w, X)), ws[-1], levels=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Linear Support Vector Machine\n",
    "\n",
    "In this section we implement a linear support vector machine using the hinge loss, but without the regularization term (we will discuss this in the next chapters):\n",
    "\n",
    "\\begin{align*}\n",
    "\tl_{\\text{hinge}}(h(x), y)\n",
    "\t & = \\max \\{0, 1-y\\cdot h(x)\\}\n",
    "\\end{align*}\n",
    "\n",
    "The objective is then\n",
    "\n",
    "\\begin{equation*}\n",
    "J(w) = \\frac{1}{m} \\sum_{i=1}^m \\max \\{0, 1-y\\cdot h(x)\\}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Open svm.py and implement the svm objective J and gradient dJ\n",
    "from mlis.classification.svm import J, dJ\n",
    "\n",
    "w = np.array([2, 3, -1])\n",
    "assert dJ(w, X, y).shape == (3,)  # we expect a vector of size (3,)\n",
    "np.testing.assert_almost_equal(check_grad(J, dJ, w, X, y), 0.0, decimal=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "w0 = np.zeros(3)  # initial weights\n",
    "eta = 1  # learning rate\n",
    "iters = 200  # number of iterations\n",
    "\n",
    "ws = gradientDescent(dJ, X, y, w0, eta, iters)\n",
    "print(f\"optimal weights w: {ws[-1]}\")\n",
    "\n",
    "plot_fit(h, ws[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "w0 = np.zeros(3)  # initial weights\n",
    "eta = 1  # learning rate\n",
    "iters = 200  # number of iterations\n",
    "\n",
    "ws = gradientDescent(dJ, X, y, w0, eta, iters)\n",
    "print(f\"optimal weights w: {ws[-1]}\")\n",
    "\n",
    "plot_fit(h, ws[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Open losses.py and implement the logistic_loss and hinge_loss\n",
    "from mlis.classification.losses import logistic_loss, hinge_loss\n",
    "\n",
    "y = np.array([-1, -1, 1, 1])\n",
    "yhat = np.array([-2, 3, -4, 1])\n",
    "\n",
    "np.testing.assert_array_almost_equal(logistic_loss(yhat, y), [0.127, 3.049, 4.018, 0.313], decimal=3)\n",
    "np.testing.assert_array_almost_equal(hinge_loss(yhat, y), [0., 4., 5., 0.], decimal=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "dc66e4307222f484e9e51e3e58e9832a25af17cc4b70bcad5e72d26d360bcade"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
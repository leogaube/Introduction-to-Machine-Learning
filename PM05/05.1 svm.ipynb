{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "revision: 78571a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# @formatter:off\n",
    "# PREAMBLE\n",
    "import cvxopt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_context(\"notebook\", font_scale=1.1)\n",
    "sns.set_style(\"ticks\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# @formatter:on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Open kernels.py and implement the function sq_exp (can be copied from previous problem set)\n",
    "from mlis.kernels.kernels import sq_exp\n",
    "\n",
    "sigma = 10\n",
    "X = np.array([[1, 2, -1], [3, 2, 1]])\n",
    "Z = np.array([[5, 1, 5]])\n",
    "# test shape\n",
    "np.testing.assert_array_equal(sq_exp(X, Z, sigma).shape, (2, 1))\n",
    "np.testing.assert_array_almost_equal(sq_exp(X, Z, sigma), [[0.767], [0.9]], decimal=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### SVM Dual Form\n",
    "\n",
    "Recall the primal support vector machine optimization problem:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min_{w,b}\\quad & \\lVert w \\rVert^2 + C \\sum_{i=1}^n \\xi_i \\\\\n",
    "\\text{s.t.}\\quad & y_i(w^Tx_i + b) \\geq 1 - \\xi_i\\\\\n",
    "& \\xi_i \\geq 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We can form the Lagrangian:\n",
    "$$\n",
    "\\mathcal{L}(w,b,\\xi,\\alpha,\\gamma) = \\frac{1}{2}w^Tw + C\\sum_{i=1}^n \\xi_i - \\sum_{i=1}^n\\alpha_i[y_i(x_i w+b) -w +\\xi_i] - \\sum_{i=1}^n\\gamma_i\\xi_i\n",
    "$$\n",
    "\n",
    "Here, the $\\alpha_i$'s and $\\gamma_i$'s are our Lagrange multipliers (constrained to be $\\geq 0$).\n",
    "\n",
    "After setting the derivatives with respect to w and b to zero as before, substituting them back in, and simplifying, we obtain the following dual form of the problem:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\max_{\\alpha} \\quad & \\sum_{i=1}^n \\alpha_i - \\frac{1}{2} \\sum_{i,j=1}^n y_iy_j\\alpha_i\\alpha_j \\langle x_i, x_j \\rangle \\\\\n",
    "\\text{s.t.}\\quad & 0\\leq \\alpha_i\\leq C\\\\\n",
    "& \\sum_{i=1}^n \\alpha_i y_i = 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We have that $w$ can be expressed in terms of the $\\alpha_i$'s given by\n",
    "$$\n",
    "w = \\sum_{i=1}^n \\alpha_i y_i x_i,\n",
    "$$\n",
    "so that after solving the dual problem, we can use\n",
    "$$\n",
    "f(x) = w^Tx+b = \\sum_{i=1}^n \\alpha_i y_i \\langle x_i, x \\rangle +b\n",
    "$$\n",
    "to make our predictions.\n",
    "\n",
    "Since the algorithm can be written entirely in terms of the inner products $\\langle x, z \\rangle$, this means that we would replace all those inner products with $\\langle \\phi(x), \\phi(z) \\rangle$. Specificically, given a feature mapping $\\phi$, we define the corresponding kernel to be\n",
    "$$\n",
    "k(x, z) = \\langle \\phi(x), \\phi(z) \\rangle.\n",
    "$$\n",
    "Then, everywhere we previously had $\\langle x, z \\rangle$ in our algorithm, we could simply replace it with $k(x, z)$, and our algorithm would now be learning using the features $\\phi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Solving the svm dual optimization problem with Quadratic Programming\n",
    "\n",
    "The SVM dual problem can be optimized by quadratic programming. You need to download and install a third party library called \"cvxopt\".\n",
    "\n",
    "Quadratic programming is a general optimization routine to find a minimum for a problem specified by\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min_x\\quad& \\frac{1}{2} x^TPx + q^Tx\\\\\n",
    "\\text{s.t.}\\quad & Gx \\leq h\\\\\n",
    "& Ax = b\n",
    "\\end{align}\n",
    "$$\n",
    "and we have to bring the SVM dual into this form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# wrapper around cvxopt\n",
    "def quadprog(P, q, G, h, A, b):\n",
    "    sol = cvxopt.solvers.qp(cvxopt.matrix(P),\n",
    "                            cvxopt.matrix(q),\n",
    "                            cvxopt.matrix(G),\n",
    "                            cvxopt.matrix(h),\n",
    "                            cvxopt.matrix(A),\n",
    "                            cvxopt.matrix(b))\n",
    "    return np.ravel(sol['x'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### The Quadratic Program\n",
    "We write the SVM optimization problem as a Quadratic Program.\n",
    "Let $K$ be the $m \\times m$ Gram matrix with $K_{i,j} = k(x_i,x_j)$ then\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P & = \\vec{y}\\vec{y}^T \\circ K \\in \\mathbb{R}^{m \\times m}\\\\\n",
    "q & = -\\vec{1} \\in \\mathbb{R}^{m}\\\\\n",
    "G & = \\begin{bmatrix} -I \\\\ I \\end{bmatrix} \\in \\mathbb{R}^{2m \\times m} \\\\\n",
    "h & = \\begin{bmatrix}\\vec{0}\\\\\\vec{1}\\cdot C\\end{bmatrix} \\in \\mathbb{R}^{2m}\\\\\n",
    "A & = \\vec{y}^T \\in \\mathbb{R}^{1 \\times m}\\\\\n",
    "b & = 0.0 \\in \\mathbb{R}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $I$ is the identity matrix of size $m\\times m$.\n",
    "\n",
    "Implement the function `lagrange_multipliers` to calculate these quantities and call `quadprog` to obtain the $\\alpha$'s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from mlis.kernels.kernel_svm import lagrange_multipliers\n",
    "\n",
    "C = 1\n",
    "y = np.array([[-1.], [1], [1.]])\n",
    "K = np.array([\n",
    "    [1., .1, .1],\n",
    "    [.1, 1., .99],\n",
    "    [.1, .99, 1],\n",
    "])\n",
    "alpha = lagrange_multipliers(y, C, K)\n",
    "np.testing.assert_array_almost_equal(alpha, [1., 0.5, 0.5], decimal=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The optimal bias $b$ can be determined by KKT dual-complementarity conditions which quarantees that\n",
    "$$\n",
    "y_i (w^Tx_i + b) = 1 \\quad\\text{if}\\quad 0 < \\alpha_i < C\n",
    "$$\n",
    "and hence\n",
    "$$\n",
    "\\begin{align}\n",
    "y_i (w^Tx_i + b) &= 1\\\\\n",
    "w^Tx_i + b &= y_i\\\\\n",
    "b &= y_i - w^Tx_i\\\\\n",
    "&= y_i - \\sum_{j=1}^n \\alpha_j y_j k(x_j, x_i)\n",
    "\\end{align}\n",
    "$$\n",
    "for any $0 < \\alpha_i < C$.\n",
    "\n",
    "Implement the function `bias` to obtain $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from mlis.kernels.kernel_svm import bias\n",
    "\n",
    "C = 1\n",
    "y = np.array([[-1.], [1], [1.]])\n",
    "K = np.array([\n",
    "    [1., .1, .1],\n",
    "    [.1, 1., .99],\n",
    "    [.1, .99, 1],\n",
    "])\n",
    "alpha = np.array([1., 0.5, 0.5])\n",
    "b = bias(y, C, alpha, K)\n",
    "\n",
    "np.testing.assert_array_almost_equal(b, 0.005, decimal=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We now have function to calculate $\\alpha$ and $b$. This allows us to implement the support vector machine. \n",
    "The function `svm_fit` first computes $\\alpha$ and $b$ and then returns a function\n",
    "$$\n",
    "z\\mapsto \\sum_{i=1}^n \\alpha_i y_i k(x_i, z) +b\n",
    "$$\n",
    "which is able to make predictions for new data point $z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from mlis.kernels.kernel_svm import svm_fit\n",
    "\n",
    "\n",
    "# Plotting decision regions\n",
    "def plot_fit(h=None, ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    sns.scatterplot(ax=ax, x='impedance', y='velocity', hue='type', data=df, palette=dict(sand='orange', shale='gray'))\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    scaler = preprocessing.StandardScaler().fit(df[['impedance', 'velocity']].values)\n",
    "    if h:\n",
    "        grid = 150\n",
    "        xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], grid), np.linspace(ylim[0], ylim[1], grid))\n",
    "        XY = np.array([np.ravel(xx), np.ravel(yy)]).T\n",
    "        XY = scaler.transform(XY)\n",
    "        P = h(XY).reshape(grid, grid)\n",
    "        cmap = sns.diverging_palette(275, 10, as_cmap=True)\n",
    "        ax.contourf(xx, yy, P, alpha=.2, cmap=cmap, levels=np.linspace(P.min(), P.max(), 20))\n",
    "        # cn = ax.contour(xx, yy, P, colors='k', levels=10)\n",
    "        cn = ax.contour(xx, yy, P, colors='k', levels=[-1,0,1])\n",
    "        ax.clabel(cn, inline=1, fontsize=10)\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "\n",
    "\n",
    "# load rock types data set\n",
    "df = pd.read_csv('rocktypes.csv', sep=' ')\n",
    "X = df[['impedance', 'velocity']].values\n",
    "y = df['type'].apply(lambda string: -1 if string == 'sand' else +1).values\n",
    "# normalize data using scipy\n",
    "X = preprocessing.StandardScaler().fit_transform(X)\n",
    "\n",
    "# learn the support vector machine\n",
    "sigma = 1.0\n",
    "C = 1\n",
    "kernel = lambda X, Z: sq_exp(X, Z, sigma)\n",
    "svm = svm_fit(X, y, C, kernel)\n",
    "\n",
    "plot_fit(svm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
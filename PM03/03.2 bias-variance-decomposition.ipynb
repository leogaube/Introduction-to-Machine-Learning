{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "still-nevada",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Bias Variance Decomposition\n",
    "\n",
    "revision: dcfbda7\n",
    "\n",
    "The goal of this exercise is to verify the bias variance decomposition for the relationship\n",
    "$$\n",
    "Y = f(X) + \\epsilon\n",
    "$$\n",
    "To ease code, we used $f$ instead of $h^{\\star}$ as in the script.\n",
    "Try to follow the code and then implement a variant of it on your own.\n",
    "\n",
    "First, we generate some data according to the forumla $Y = f(X) + \\epsilon$, where we will use $\\epsilon \\sim N(0, \\text{noise_std}^2)$, i.e. $\\epsilon$ will be normal distributed with standard deviation `noise_std`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-newman",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "rng = np.random.RandomState(3)  # fix rng\n",
    "m = 10  # number of observations\n",
    "noise_std = 0.5  # standard deviation of N(0,std) noise\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    \"\"\"True function $h^{\\star}$\"\"\"\n",
    "    return np.sin(x * np.pi)\n",
    "\n",
    "\n",
    "def sample_data(x, noise_std, rng):\n",
    "    \"\"\"$Y = h^{\\star}(X) + \\epsilon$\"\"\"\n",
    "    return f(x) + rng.randn(*x.shape) * noise_std\n",
    "\n",
    "\n",
    "# create data set\n",
    "t = np.linspace(-1, 1, 100)\n",
    "X = 2 * (rng.rand(m) - 0.5)\n",
    "y = sample_data(X, noise_std, rng)\n",
    "\n",
    "plt.plot(t, f(t), color='k', label='f(x)')\n",
    "plt.scatter(X, y, label='data')\n",
    "plt.xlim([-1, 1])\n",
    "plt.ylim([-2, 2])\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prerequisite-camping",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def ridge_poly(X, y, lam, degree=m):\n",
    "    \"\"\"Polynomial ridge regression\"\"\"\n",
    "\n",
    "    def poly_feat(X, degree):\n",
    "        return np.array([X ** i for i in range(degree, -1, -1)]).T\n",
    "\n",
    "    X = poly_feat(X, degree)\n",
    "    w = np.linalg.solve(np.dot(X.T, X) + np.eye(degree + 1) * lam, np.dot(X.T, y))\n",
    "    return lambda X: np.dot(poly_feat(X, degree), w.T)\n",
    "\n",
    "\n",
    "def sq_error(ypred, y):\n",
    "    \"\"\"squared error\"\"\"\n",
    "    return np.mean((ypred - y) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-antibody",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Each model fit is based on a particular training set. Depending on the regularization, we observe under- or overfitting. To get an idea of how each of the models depend on particular data, we sample 50 independed sets and fit a model to each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silent-adams",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(3)  # fix rng\n",
    "COLORS = ['orange', 'royalblue', 'darkgreen', 'darkred']\n",
    "runs = 50\n",
    "lambdas = [0, .0001, .1, 1]  # regularization parameters\n",
    "\n",
    "yhats = defaultdict(list)\n",
    "for run in range(50):\n",
    "    y = sample_data(X, noise_std, rng)\n",
    "    for lam in lambdas:\n",
    "        h = ridge_poly(X, y, lam)\n",
    "        yhats[lam].append(h(t))\n",
    "\n",
    "fig, axs = plt.subplots(1, len(lambdas), figsize=(15, 5))\n",
    "for i, lam in enumerate(lambdas):\n",
    "    plt.sca(axs[i])\n",
    "    # plot individual fits\n",
    "    for j, yhat in enumerate(yhats[lam]):\n",
    "        plt.plot(t, yhat, alpha=.1, color=COLORS[i])\n",
    "    # plot mean\n",
    "    Eh = np.mean(yhats[lam], axis=0)  # E[h(X)]\n",
    "    plt.plot(t, Eh, color=COLORS[i], linewidth=3)\n",
    "    # plot true function\n",
    "    plt.plot(t, f(t), alpha=.4, color='k', label='f(x)')\n",
    "    plt.xlim([-1, 1])\n",
    "    plt.ylim([-2, 2])\n",
    "    plt.title(f'$\\lambda$:{lam} loss:{sq_error(Eh, f(t)):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forty-ground",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Based on the samples above, we now decompose the generalization error into bias, variance and noise.\n",
    "If we are correct, then these values add up to the generalization error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-there",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)  # fix rng\n",
    "\n",
    "m = 35  # data set size\n",
    "noise_std = 0.5  # noise standard deviation\n",
    "runs = 200  # number of runs\n",
    "n_train = int(np.ceil(m * .8))  # number of training points\n",
    "\n",
    "# create training and test inputs (same for each run)\n",
    "x = rng.permutation(np.linspace(-1, 1, m))\n",
    "x_train = x[:n_train]\n",
    "x_test = x[n_train:]\n",
    "\n",
    "# create new training and test labels for each run\n",
    "y_train = [sample_data(x_train, noise_std, rng) for _ in range(runs)]\n",
    "y_test = [sample_data(x_test, noise_std, rng) for _ in range(runs)]\n",
    "\n",
    "# model complexities to use\n",
    "lambdas = np.exp(np.linspace(np.log(1e-10), np.log(1), 100))\n",
    "\n",
    "\n",
    "def bias_var_decomp(lam):\n",
    "    '''calculate bias variance decomposition based on regularization term lambda'''\n",
    "    pred_train = np.zeros((runs, len(x_train)))\n",
    "    pred_test = np.zeros((runs, len(x_test)))\n",
    "    # average over runs\n",
    "    for run in range(runs):\n",
    "        # train model\n",
    "        h = ridge_poly(x_train, y_train[run], lam)\n",
    "\n",
    "        # make predictions on train set\n",
    "        pred_train[run, :] = h(x_train)\n",
    "\n",
    "        # make predictions on test set\n",
    "        pred_test[run, :] = h(x_test)\n",
    "\n",
    "    Eh = np.array(pred_test).mean(0)  # E[h(X)]\n",
    "    bias = np.mean((Eh - f(x_test)) ** 2)  # (E[h(X)] - hstar(X))^2\n",
    "    variance = np.mean((Eh - pred_test) ** 2)  # (E[h(X)] - h(X))^2\n",
    "    train_error = sq_error(pred_train, y_train)\n",
    "    test_error = sq_error(pred_test, y_test)\n",
    "    return bias, variance, train_error, test_error\n",
    "\n",
    "\n",
    "train_errors = np.zeros((len(lambdas),))\n",
    "test_errors = np.zeros((len(lambdas),))\n",
    "biases = np.zeros((len(lambdas),))\n",
    "variances = np.zeros((len(lambdas),))\n",
    "\n",
    "# loop over model complexities\n",
    "for i, lam in enumerate(lambdas):\n",
    "    biases[i], variances[i], train_errors[i], test_errors[i] = bias_var_decomp(lam)\n",
    "\n",
    "# create plots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 8))\n",
    "\n",
    "## bias^2 + variance\n",
    "plt.sca(axs[0])\n",
    "plt.plot(lambdas, biases, color='seagreen', label='$bias^2$')\n",
    "plt.plot(lambdas, variances, color='royalblue', label='variance')\n",
    "plt.plot(lambdas, biases + variances + noise_std ** 2, linestyle='-.', color='gray',\n",
    "         label='$bias^2 + variance + V[\\epsilon]$')\n",
    "plt.plot(lambdas, test_errors, label='generalization error', color='orange')\n",
    "plt.axhline(noise_std ** 2, color='tomato', linestyle='--', label=f'$V[\\epsilon]$ = {round(noise_std ** 2, 3)}')\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel('error')\n",
    "plt.ylim([0, .6]);\n",
    "plt.legend(loc='upper center')\n",
    "\n",
    "## train / test error\n",
    "plt.sca(axs[1])\n",
    "plt.plot(lambdas, train_errors, label='train error', color='royalblue')\n",
    "plt.plot(lambdas, test_errors, label='generalization error', color='orange')\n",
    "plt.ylim([0, .6]);\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel('error')\n",
    "plt.legend(loc='upper center');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informal-branch",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Question: What is the generalization error of the best possible model $h^\\star$ (named $f$ here in the code)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-thesaurus",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Task: Perfom the same analysis for unregularized polynomial linear regression. That means your model complexity is the polynomial degree instead of $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-stylus",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)  # fix rng\n",
    "\n",
    "m = 35  # data set size\n",
    "noise_std = 0.5  # noise standard deviation\n",
    "runs = 200  # number of runs\n",
    "n_train = int(np.ceil(m * .8))  # number of training points\n",
    "\n",
    "# create training and test inputs (same for each run)\n",
    "x = rng.permutation(np.linspace(-1, 1, m))\n",
    "x_train = x[:n_train]\n",
    "x_test = x[n_train:]\n",
    "\n",
    "# create new training and test labels for each run\n",
    "y_train = [sample_data(x_train, noise_std, rng) for _ in range(runs)]\n",
    "y_test = [sample_data(x_test, noise_std, rng) for _ in range(runs)]\n",
    "\n",
    "# model complexities to use\n",
    "degrees = np.arange(1, 12)\n",
    "\n",
    "\n",
    "def bias_var_decomp(degree):\n",
    "    '''calculate bias variance decomposition based on regularization term lambda'''\n",
    "    pred_train = np.zeros((runs, len(x_train)))\n",
    "    pred_test = np.zeros((runs, len(x_test)))\n",
    "    # average over runs\n",
    "    for run in range(runs):\n",
    "        # train model\n",
    "        h = ls_poly(x_train, y_train[run], degree)\n",
    "\n",
    "        # make predictions on train set\n",
    "        pred_train[run, :] = h(x_train)\n",
    "\n",
    "        # make predictions on test set\n",
    "        pred_test[run, :] = h(x_test)\n",
    "\n",
    "    Eh = np.array(pred_test).mean(0)  # E[h(X)]\n",
    "    bias = np.mean((Eh - f(x_test)) ** 2)  # (E[h(X)] - hstar(X))^2\n",
    "    variance = np.mean((Eh - pred_test) ** 2)  # (E[h(X)] - h(X))^2\n",
    "    train_error = sq_error(pred_train, y_train)\n",
    "    test_error = sq_error(pred_test, y_test)\n",
    "    return bias, variance, train_error, test_error\n",
    "\n",
    "\n",
    "train_errors = np.zeros((len(degrees),))\n",
    "test_errors = np.zeros((len(degrees),))\n",
    "biases = np.zeros((len(degrees),))\n",
    "variances = np.zeros((len(degrees),))\n",
    "\n",
    "# loop over model complexities\n",
    "for i, degree in enumerate(degrees):\n",
    "    biases[i], variances[i], train_errors[i], test_errors[i] = bias_var_decomp(degree)\n",
    "\n",
    "# create plots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 8))\n",
    "\n",
    "## bias^2 + variance\n",
    "plt.sca(axs[0])\n",
    "plt.plot(degrees, biases, color='seagreen', label='$bias^2$')\n",
    "plt.plot(degrees, variances, color='royalblue', label='variance')\n",
    "plt.plot(degrees, biases + variances + noise_std ** 2, linestyle='-.', color='gray',\n",
    "         label='$bias^2 + variance + V[\\epsilon]$')\n",
    "plt.plot(degrees, test_errors, label='generalization error', color='orange')\n",
    "plt.axhline(noise_std ** 2, color='tomato', linestyle='--', label=f'$V[\\epsilon]$ = {round(noise_std ** 2, 3)}')\n",
    "plt.xlabel('degree')\n",
    "plt.ylabel('error')\n",
    "plt.ylim([0, .6]);\n",
    "plt.legend(loc='upper center')\n",
    "\n",
    "## train / test error\n",
    "plt.sca(axs[1])\n",
    "plt.plot(degrees, train_errors, label='train error', color='royalblue')\n",
    "plt.plot(degrees, test_errors, label='generalization error', color='orange')\n",
    "plt.ylim([0, .6]);\n",
    "plt.xlabel('degree')\n",
    "plt.ylabel('error')\n",
    "plt.legend(loc='upper center');"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
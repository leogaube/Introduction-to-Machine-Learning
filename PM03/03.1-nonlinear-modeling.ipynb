{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Nonlinear Modeling\n",
    "\n",
    "revision: dcfbda7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# @formatter:off\n",
    "# PREAMBLE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.optimize import check_grad\n",
    "from scipy.optimize import minimize\n",
    "from functools import partial\n",
    "from mlis.arrays import aslabel, asinput\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_context(\"notebook\", font_scale=1.1)\n",
    "sns.set_style(\"ticks\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# @formatter:on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "df = pd.read_csv('energy.csv', sep=' ')\n",
    "sns.lmplot(x='temp', y='energy', data=df, fit_reg=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In order to do better, we need a way to expressing the nonlinear relationship between the inputs and outputs.\n",
    "The python function `poly_feat` maps a given input to a polynomial of a certain degree. For example\n",
    "$$\n",
    "\\text{temperature} \\mapsto \\begin{bmatrix}1 \\\\ \\text{temperature} \\\\ \\text{temperature}^2\\end{bmatrix}\n",
    "$$\n",
    "if degree is equal to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Open features.py and implement the function poly_feat\n",
    "from mlis.nonlinear.features import poly_feat\n",
    "\n",
    "# use dummy data\n",
    "X = asinput([1, 2, -1])\n",
    "# test shape\n",
    "np.testing.assert_array_equal(poly_feat(X, 3).shape, (3, 4))\n",
    "# test first entry\n",
    "expected = np.array([[1., 1., 1., 1.],\n",
    "                     [1., 2., 4., 8.],\n",
    "                     [1., -1., 1., -1.]])\n",
    "np.testing.assert_array_almost_equal(poly_feat(X, 3), expected, decimal=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Write a function `ls_poly` which learns a model $h$ with the closed-form least squares algorithm.\n",
    "\n",
    "1. Project the inputs into a polynomial feature space using the `poly_feat` function (this is our $\\phi$).\n",
    "2. Determine the optimal weights $w = \\big(\\phi(X)^T\\phi(X) \\big)^{-1}\\phi(X)^Ty$.\n",
    "3. Return a *function* $h\\colon X \\mapsto w^{T}\\phi(X)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Open nonlinear_regression.py and implement the function ls_poly\n",
    "\n",
    "from mlis.nonlinear.nonlinear_regression import ls_poly\n",
    "\n",
    "X = asinput([1, 2, 4, 5, 6, 7])\n",
    "y = aslabel([1, 2 ** 3, 4 ** 3, 5 ** 3, 6 ** 3, 7 ** 3])\n",
    "h = ls_poly(X, y, degree=3)\n",
    "\n",
    "np.testing.assert_array_almost_equal(h(asinput([1, 2])), [1, 8], decimal=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We now perform regression with polynomials of different degrees. This should reproduce the plots in figure 4.4 on page 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_fit(h):\n",
    "    x = np.linspace(df['temp normalized'].min(), df['temp normalized'].max(), 100)\n",
    "    y = np.squeeze(h(x))\n",
    "    sns.lmplot(x='temp normalized', y='energy', data=df, fit_reg=False)\n",
    "    ylim = plt.ylim()\n",
    "    plt.plot(x, y, 'k', label='Prediction')\n",
    "    plt.ylim(ylim)\n",
    "\n",
    "\n",
    "# with polynomials features is best to work with data between -1 and 1.\n",
    "df['temp normalized'] = 2 * (df['temp'] - df['temp'].min()) / (df['temp'].max() - df['temp'].min()) - 1\n",
    "\n",
    "# load the data\n",
    "X = df['temp normalized'].values\n",
    "y = df['energy'].values\n",
    "\n",
    "# plot the linear regression with different polynomial degrees\n",
    "for degree in [2, 5, 9, 100]:\n",
    "    plot_fit(ls_poly(X, y, degree=degree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Cross Validation\n",
    "\n",
    "The simplest of these techniques is the *holdout set* method.\n",
    "Here we randomly split our data into two groups, the new *training set* and the *holdout* or *validation set*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def split(df, fraction, rng):\n",
    "    \"\"\"\n",
    "    given a dataframe df, split it into `fraction` training and `1-fraction` cv instances.\n",
    "    \"\"\"\n",
    "    # get a list of indices and permute them\n",
    "    perm = rng.permutation(len(df))\n",
    "    # create indices for train and cv splits\n",
    "    idx_train = perm[:int(len(perm) * fraction)]\n",
    "    idx_cv = perm[int(len(perm) * fraction):]\n",
    "    # return actual data\n",
    "    train = df.iloc[idx_train]\n",
    "    cv = df.iloc[idx_cv]\n",
    "    return train, cv\n",
    "\n",
    "\n",
    "# always seed your random number generator to have reproducible results\n",
    "rng = np.random.RandomState(5)\n",
    "train, cv = split(df, 0.7, rng)\n",
    "\n",
    "# plot splits\n",
    "fig, ax = plt.subplots()\n",
    "sns.scatterplot(ax=ax, x='temp normalized', y='energy', data=train)\n",
    "sns.scatterplot(ax=ax, x='temp normalized', y='energy', data=cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Reproduce the \"polynomial degree vs. mean squared error\" plots in figure 4.8 on page 32\n",
    "Use a function which calculates the mean squared error on the training set `train` and on the validation set `cv` depending on the polynomial degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "degrees = range(30)\n",
    "# insert your code here\n",
    "err_train = None\n",
    "err_cv = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 7))\n",
    "ax1.semilogy(degrees, err_train, degrees, err_cv)\n",
    "ax1.legend([\"Training\", \"Validation\"])\n",
    "ax1.set_xlabel(\"Polynomial degree\")\n",
    "ax1.set_ylabel(\"Mean squared error\")\n",
    "\n",
    "ax2.semilogy(degrees[:15], err_train[:15], degrees[:15], err_cv[:15])\n",
    "ax2.legend([\"Training\", \"Validation\"])\n",
    "ax2.set_xlabel(\"Polynomial degree\")\n",
    "ax2.set_ylabel(\"Mean squared error\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Regularization\n",
    "\n",
    "For far, we used the degree of the polynomial feature function as a tool to control model complexity.\n",
    "Recall, that the degree 100 polynomial overfits the peak demand data since it tries to pass through many of the data points with the aim to minimizing the objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Plot the absolute weights/values of the polynomial coefficients of a degree 100 polynomial fitted to the peak d data.\n",
    "This should reproduce figure 4.15 on page 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "X = df['temp normalized'].values\n",
    "y = df['energy'].values\n",
    "\n",
    "# insert your code here\n",
    "w = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.semilogy(range(101), np.abs(w))\n",
    "plt.xlabel('coefficient degree');\n",
    "plt.ylabel('absolute coefficient value');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Implement the closed form solution of ridge regression to obtain the optimal parameters.\n",
    "Use $\\lambda=8.0$ and a polynomial degree of $100$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# insert your code here\n",
    "w_ridge = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Plot the coefficient weights of the regularized and unregularized weights.\n",
    "This should reproduce figure 4.17 on page 40."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.semilogy(range(101), np.abs(w), range(101), np.abs(w_ridge))\n",
    "plt.xlabel('coefficient degree');\n",
    "plt.ylabel('absolute coefficient weight');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Ridge Regression\n",
    "\n",
    "Implement polynomial ridge regression in the function `ridge_poly`.\n",
    "\n",
    "1. Project the the inputs into a polynomial feature space using the `poly_feat` function (this is our $\\phi$).\n",
    "2. Determine the optimal weights $w = \\big(\\phi(X)^T\\phi(X) + \\lambda I\\big)^{-1}\\phi(X)^Ty$.\n",
    "3. Return a *function* $h\\colon X \\mapsto w^{T}\\phi(X)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Open nonlinear_regression.py and implement the function ridge_poly\n",
    "\n",
    "from mlis.nonlinear.nonlinear_regression import ridge_poly\n",
    "\n",
    "X = asinput([1, 2, 4, 5, 6, 7])\n",
    "y = aslabel([1, 2 ** 3, 4 ** 3, 5 ** 3, 6 ** 3, 7 ** 3])\n",
    "h = ridge_poly(X, y, degree=3, lam=1e6)\n",
    "\n",
    "np.testing.assert_array_almost_equal(h(asinput([1, 2])), [0.184, 1.344], decimal=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Reproduce the \"$\\lambda$ vs. mean squared error\" plot figure 4.18 on page 40.\n",
    "Use a function which calculates the mean squared error on the training set `train` and on the validation set `cv` depending on $\\lambda$.\n",
    "The polynomial degree is fixed to $100$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lambdas = np.logspace(-15, 10, 100)\n",
    "# insert your code here\n",
    "err_train = None\n",
    "err_cv = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 7))\n",
    "ax1.loglog(lambdas, err_train, lambdas, err_cv)\n",
    "ax1.legend([\"Training\", \"Validation\"])\n",
    "ax1.set_xlabel(\"Regularization parameter $\\lambda$\")\n",
    "ax1.set_ylabel(\"Mean squared error\")\n",
    "\n",
    "ax2.loglog(lambdas[45:75], err_train[45:75], lambdas[45:75], err_cv[45:75])\n",
    "ax2.legend([\"Training\", \"Validation\"])\n",
    "ax2.set_xlabel(\"Regularization parameter $\\lambda$\")\n",
    "ax2.set_ylabel(\"Mean squared error\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lasso Regression\n",
    "\n",
    "Similar to ridge regression, there’s another regularized regression technique called \\define{lasso regression} which uses $\\ell_1$-regularization, ie $\\Omega(w) = \\sum_{i=1}^{n} |w_i| = ||{w}||_1$.\n",
    "The only difference is that we are taking the absolute value instead of the square of all\n",
    "the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Implement $J$ and $\\nabla J$ for Lasso regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Open lasso_regression.py and implement the functions J and dJ\n",
    "\n",
    "from mlis.nonlinear.lasso_regression import J, dJ\n",
    "\n",
    "X = asinput([1, 2, 4, 5, 6, 7])\n",
    "y = aslabel([1, 2 ** 3, 4 ** 3, 5 ** 3, 6 ** 3, 7 ** 3])\n",
    "w = [-0.008, -0.02, 0.02, 1]\n",
    "degree = 3\n",
    "lam = 8\n",
    "\n",
    "np.testing.assert_array_almost_equal(J(w, X, y, degree, lam), 8.59, decimal=2)\n",
    "np.testing.assert_almost_equal(check_grad(J, dJ, w, X, y, degree, lam), 0.0, decimal=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "X = df['temp normalized'].values\n",
    "y = df['energy'].values\n",
    "lam = 7 / X.shape[0]\n",
    "degree = 100\n",
    "\n",
    "# we do not implement gradient descent again, but use scipy.optimize\n",
    "# have a look at the documentation of this function to understand what we are doing here.\n",
    "w_lasso = minimize(J, np.zeros((degree + 1,)), args=(X, y, degree, lam), jac=dJ, method='CG').x\n",
    "h = lambda X: np.dot(poly_feat(X, degree), w_lasso.T)\n",
    "\n",
    "plot_fit(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Reproduce figure 4.19 on page 41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# plot the lasso weights. Make sure that most of them are (close to) zero.\n",
    "plt.figure()\n",
    "plt.semilogy(np.abs(w_lasso))\n",
    "plt.xlabel(\"Degree of coefficient\")\n",
    "plt.ylabel(\"Coefficient weight\");"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "dc66e4307222f484e9e51e3e58e9832a25af17cc4b70bcad5e72d26d360bcade"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
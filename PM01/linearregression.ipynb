{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Linear Regression\n",
    "\n",
    "revision: dcfbda7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# PREAMBLE\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.optimize import check_grad\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_context(\"notebook\", font_scale=1.1)\n",
    "sns.set_style(\"ticks\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load all data\n",
    "df = pd.read_csv('energy_summer_small.csv', sep=' ', index_col='Date')\n",
    "df.index = pd.to_datetime(df.index)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Exploring the data\n",
    "\n",
    "Before starting on any task, it is often useful to understand the data by visualizing it. In this example it can be done using a scatter plot since we only have two variables (temperature and load)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.scatterplot(ax=ax, x='temp', y='energy', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Feature Scaling\n",
    "\n",
    "Scale the temperature column such that it has zero mean and unit variance. It is best not to overwrite `df['temp']`, but to create a new column `df['temp normalized']`.\n",
    "Compare this to figure 1.4 on page 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df['temp normalized'] = 1  # insert your code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 2, figsize=(16, 4))\n",
    "sns.scatterplot(ax=ax[0], x='temp', y='energy', data=df)\n",
    "sns.scatterplot(ax=ax[1], x='temp normalized', y='energy', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Bias Term\n",
    "In order to facilitate the vector notation $w^Tx$ we add a bias term to the vector $x$ i.e. an element with the value $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# add bias term\n",
    "df['bias'] = 1\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# export pandas dataframe to numpy array\n",
    "X = df[['bias', 'temp normalized']].values\n",
    "y = df['energy'].values\n",
    "m, n = X.shape\n",
    "\n",
    "print(f\"Dimensions X: {X.shape}, y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Open mlis/regression/linearregression.py and implement the functions h and J.\n",
    "from mlis.regression.linearregression import h, J\n",
    "\n",
    "# test the hypothesis and cost function for w = [2,3]\n",
    "w = np.array([2, 3])\n",
    "# test the first 3 values calculated by h\n",
    "np.testing.assert_array_almost_equal(h(w, X)[:3], [-0.76, -0.08, -0.08], decimal=2)\n",
    "# test the correctness of J\n",
    "np.testing.assert_array_almost_equal(J(w, X, y), 8.54, decimal=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Since $w$ is only two-dimensional, we can visualize $J(w)$ with a surface plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_objective_contours(X, y, xlim=(0, 5), ylim=(-1.5, 2)):\n",
    "    XX, YY = np.meshgrid(np.linspace(*xlim, 200), np.linspace(*ylim, 200))\n",
    "    W = np.hstack([np.ravel(XX)[:, None], np.ravel(YY)[:, None]])\n",
    "    ZZ = [J(w, X, y) for w in W]\n",
    "    ZZ = np.array(ZZ).reshape(XX.shape)\n",
    "\n",
    "    plt.contour(XX, YY, ZZ, levels=10)\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    plt.xlabel(\"$w_1$\")\n",
    "    plt.ylabel(\"$w_2$\")\n",
    "\n",
    "\n",
    "plot_objective_contours(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Implementing gradient descent\n",
    "\n",
    "Now that we have the hypothesis and the cost function we can implement the gradient descent routine. First, write a function $\\nabla J(w)$ which returns the gradient of the cost function. The gradient is just a vector with all the partial derivatives\n",
    "$$\n",
    "\\nabla J(w) = \\bigg(\\frac{\\partial J(w)}{\\partial w_1} , \\dotsc, \\frac{\\partial J(w)}{\\partial w_n} \\bigg)\n",
    "$$\n",
    "The gradient of the least squares objective is\n",
    "$$\n",
    "\\nabla J(w) = \\frac{2}{m}X^{\\intercal}(Xw-y)\n",
    "$$\n",
    "when written in matrix-form (compare this to the script)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Open mlis/regression/linearregression.py and implement the function dJ.\n",
    "from mlis.regression.linearregression import dJ\n",
    "\n",
    "# test the gradient for w = [2,3]\n",
    "w = np.array([2, 3])\n",
    "assert dJ(w, X, y).shape == (n,)  # we expect a vector of size (n,)\n",
    "np.testing.assert_almost_equal(check_grad(J, dJ, w, X, y), 0.0, decimal=3)  # test the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Implement the gradient descent update rule\n",
    "$$\n",
    "w \\leftarrow w - \\eta \\nabla J(w)\n",
    "$$\n",
    "\n",
    "In order to inspect the performance of the algorithm it is necessary to store the weights $w$ at each iteration $t = 1,\\dots,iters$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Open mlis/regression/linearregression.py and implement the function gradientDescent\n",
    "\n",
    "from mlis.regression.linearregression import gradientDescent\n",
    "\n",
    "w0 = [1, 1]  # initial weights\n",
    "eta = 0.05  # learning rate Î·\n",
    "iters = 100  # number of iterations\n",
    "\n",
    "ws = gradientDescent(dJ, X, y, w0, eta, iters)\n",
    "# the last entry contains the final parameters found by gradient decent\n",
    "print(f\"best weights w: {ws[-1]}\")\n",
    "\n",
    "# testing the correctness of gradientDescent\n",
    "np.testing.assert_array_almost_equal(ws[-1], [3.1168709, 0.27646542], decimal=4)  # test optimal weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Verifying Gradient Descent\n",
    "\n",
    "A good way to verify that gradient descent is working correctly is to look at the values $J(w)$ and check that it is decreasing with each step.\n",
    "Compare the following plot to figure 1.5 on page 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_progress(costs, log=True):\n",
    "    with sns.axes_style(\"darkgrid\"):\n",
    "        _, ax = plt.subplots()\n",
    "        ax.plot(np.arange(len(costs)), costs, 'k')\n",
    "        ax.set_xlabel('Iterations')\n",
    "        ax.set_ylabel('J(w)')\n",
    "        if log:\n",
    "            ax.set_yscale('log')\n",
    "        return ax\n",
    "\n",
    "\n",
    "plot_progress([J(w, X, y) for w in ws]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# show the path gradient descent took on the objective contours\n",
    "plot_objective_contours(X, y)\n",
    "plt.plot(ws[:, 0], ws[:, 1], '-ko');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Show the model that corresponds to the learned parameters.\n",
    "Compare this to the plot in the lecture notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_fit(h, w, normalized=True):\n",
    "    outputs = 'energy'\n",
    "    inputs = 'temp normalized' if normalized else 'temp'\n",
    "    hue = None\n",
    "\n",
    "    t = np.linspace(df[inputs].min(), df[inputs].max(), 100)\n",
    "    x = np.vstack([np.ones(t.shape), t]).T  # add bias term\n",
    "    yh = np.squeeze(h(w, x))\n",
    "\n",
    "    _, ax = plt.subplots()\n",
    "    sns.scatterplot(ax=ax, x=inputs, y=outputs, hue=hue, data=df)\n",
    "    ax.plot(t, yh, 'k', label='Prediction')\n",
    "\n",
    "\n",
    "plot_fit(h, ws[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Omitting Feature Scaling\n",
    "\n",
    "Plot the objective and gradient descent progress without feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X = df[['bias', 'temp', ]].values\n",
    "y = df['energy'].values\n",
    "\n",
    "# Try different values for eta and iters.\n",
    "# With the following defaults, you will get numerical errors.\n",
    "w0 = (4, 1)\n",
    "ws = gradientDescent(dJ, X, y, w0, eta=0.001, iters=100)\n",
    "print(f\"calculated weights: {ws[-1]}\")\n",
    "print(f\"optimal weights:    [ 0.3587, 0.1127]\")\n",
    "\n",
    "# plot objective contours\n",
    "plot_objective_contours(X, y, xlim=[-1, 5], ylim=[-2, 2])\n",
    "\n",
    "# plot only 100 weights\n",
    "idx = np.linspace(0, len(ws) - 1, 100).astype(int)\n",
    "plt.plot(ws[idx, 0], ws[idx, 1], '-ko')\n",
    "\n",
    "# plot the objective values for the weights on a log scale\n",
    "_, ax = plt.subplots()\n",
    "ax.plot(idx, [J(w, X, y) for w in ws[idx, :]])\n",
    "ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Analytic Solution to Least Squares\n",
    "\n",
    "We know that there exists a closed-form solution to linear regression which is given by\n",
    "$$\n",
    "w^{\\star} = \\big(X^TX \\big)^{-1}X^Ty\n",
    "$$\n",
    "Using this formula does not require any feature scaling, and you will get an exact solution in one calculation: there is no \"loop until convergence\" like in gradient descent. However, it can only be used with the squared loss.\n",
    "\n",
    "Important: In general, one should avoid calculating matrix inverses as this leads to numerical problems. Instead, use `np.linalg.solve` to solve the system of linear equations $X^TXw=X^Ty$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Open mlis/regression/linearregression.py and implement the function least_squares\n",
    "\n",
    "from mlis.regression.linearregression import least_squares\n",
    "\n",
    "X = df[['bias', 'temp', ]].values\n",
    "y = df['energy'].values\n",
    "w = least_squares(X, y)\n",
    "\n",
    "# testing the correctness of least_squares\n",
    "np.testing.assert_array_almost_equal(least_squares(X, y), [0.3587, 0.1127], decimal=4)  # test optimal weights\n",
    "\n",
    "# plot the model\n",
    "plot_fit(h, w, normalized=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Additional Features\n",
    "We now use the additional input feature from the data set column `non_working`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "sns.scatterplot(ax=ax, x='temp', y='energy', hue='non_working', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Create a column `df[\"is_workday\"]`, which is `0` if the column `non_working` has entry `non-working` and 1` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# add the weekday column to our data frame\n",
    "df[\"is_workday\"] = True  # Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df[\"is_workday\"] = (df[\"non_working\"] == 'working') * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# the scatter plot should look the same as above, just with a 0/1 legend\n",
    "_, ax = plt.subplots()\n",
    "sns.scatterplot(ax=ax, x='temp', y='energy', hue='is_workday', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Calculate the optimal weights for the model with the additional `non_working` feature using the closed-form solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# insert your code here.\n",
    "X = None\n",
    "y = None\n",
    "w = None\n",
    "print(f\"optimal weights w: {w}\")\n",
    "# expected is [0.28023641 0.25347537 0.10915125]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X = df[['bias', 'is_workday', 'temp']].values\n",
    "y = df['energy'].values\n",
    "w = least_squares(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_almost_equal(least_squares(X, y), (0.2802, 0.2535, 0.1092), decimal=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Visualize the model fit by plotting the resulting predictions for both the weekend and weekday cases. Your plot should be identical to figure 2.2 on page 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# insert your code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "sns.scatterplot(ax=ax, x='temp', y='energy', hue='is_workday', data=df)\n",
    "t = np.linspace(df['temp'].min(), df['temp'].max(), 100)\n",
    "\n",
    "# workday predictions\n",
    "x = np.vstack([np.ones(t.shape), np.ones(t.shape), t]).T\n",
    "y = np.squeeze(h(w, x))\n",
    "plt.plot(t, y, 'r', label='Prediction')\n",
    "\n",
    "# non-workday predictions\n",
    "x = np.vstack([np.ones(t.shape), np.zeros(t.shape), t]).T\n",
    "y = np.squeeze(h(w, x))\n",
    "\n",
    "plt.plot(t, y, 'b', label='Prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Create Features\n",
    "$$\n",
    "   x = \\begin{bmatrix}\n",
    "   \\text{is_workday}\\\\\n",
    "   1-\\text{is_workday}\\\\\n",
    "   \\text{temp} \\cdot \\text{is_workday}\\\\\n",
    "   \\text{temp} \\cdot (1-\\text{is_workday})\n",
    "   \\end{bmatrix}\n",
    "$$\n",
    "and reproduce figure 2.3 on page 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# insert your code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X = np.vstack(\n",
    "    (df['is_workday'], 1 - df['is_workday'], df['is_workday'] * df['temp'], (1 - df['is_workday']) * df['temp'])).T\n",
    "y = df['energy'].values\n",
    "w = least_squares(X, y)\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "sns.scatterplot(ax=ax, x='temp', y='energy', hue='is_workday', data=df)\n",
    "t = np.linspace(df['temp'].min(), df['temp'].max(), 100)\n",
    "\n",
    "# workday predictions\n",
    "x = np.vstack([np.ones(t.shape), t]).T\n",
    "y = np.squeeze(h([w[0], w[2]], x))\n",
    "plt.plot(t, y, 'r', label='Prediction')\n",
    "\n",
    "# non-workday predictions\n",
    "y = np.squeeze(h([w[1], w[3]], x))\n",
    "plt.plot(t, y, 'b', label='Prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Loss Functions for Regression\n",
    "\n",
    "The goal of this section is to compare the absolute and squared loss on a small data sets with an outlier.\n",
    "First, we load the data a plot the squared loss result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('energy_summer_small.csv', sep=' ', index_col='Date')\n",
    "# add an outlier to the data set\n",
    "df['outlier'] = 'normal'\n",
    "outlier = pd.DataFrame(\n",
    "    dict(\n",
    "        Date='1900-01-01',\n",
    "        temp=35,\n",
    "        energy=2.7,\n",
    "        outlier='outlier',\n",
    "        weekday='Sunday',\n",
    "        season='summer',\n",
    "        non_working='working',\n",
    "        split='train',\n",
    "    ),\n",
    "    index=[0]).set_index('Date')\n",
    "df = pd.concat([df, outlier])\n",
    "\n",
    "# normalize inputs\n",
    "df['temp normalized'] = (df['temp'] - df['temp'].mean()) / df['temp'].std()\n",
    "# add bias term\n",
    "df['bias'] = 1\n",
    "\n",
    "# data frame to matrix\n",
    "X = df[['bias', 'temp normalized']].values\n",
    "y = df['energy'].values\n",
    "\n",
    "# run regression with squared loss\n",
    "ws = gradientDescent(dJ, X, y, w0=np.zeros((2,)), eta=0.1, iters=100)\n",
    "w_squared_loss = ws[-1]\n",
    "plot_fit(h, w_squared_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Robust Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Open mlis/regression/robustregression.py and implement the functions J, dJ.\n",
    "from mlis.regression.robustregression import J, dJ\n",
    "\n",
    "w = np.array([2, 3])\n",
    "\n",
    "# test the correctness of J\n",
    "np.testing.assert_array_almost_equal(J(w, X, y), 2.27, decimal=2)\n",
    "\n",
    "assert dJ(w, X, y).shape == (n,)  # we expect a vector of size (n,)\n",
    "np.testing.assert_almost_equal(check_grad(J, dJ, w, X, y), 0.0, decimal=3)  # test the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# run regression with absolute loss loss\n",
    "ws = gradientDescent(dJ, X, y, w0=np.zeros((2,)), eta=0.1, iters=100)\n",
    "w_abs_loss = ws[-1]\n",
    "plot_fit(h, w_abs_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

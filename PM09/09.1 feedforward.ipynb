{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Neural Networks and Automatic Differentiation\n",
    "\n",
    "revision: 78571a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# @formatter:off\n",
    "# PREAMBLE\n",
    "\n",
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from autograd import grad\n",
    "from autograd.misc.optimizers import adam\n",
    "from numpy import testing\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from mlis.arrays import asinput, aslabel\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_context(\"notebook\", font_scale=1.1)\n",
    "sns.set_style(\"ticks\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# @formatter:on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_csv('energy.csv', sep=' ')\n",
    "X = asinput(df['temp'])\n",
    "y = aslabel(df['energy'])\n",
    "t = np.linspace(X.min(), X.max())\n",
    "\n",
    "# plot data\n",
    "fig, ax = plt.subplots()\n",
    "sns.scatterplot(ax=ax, x='temp', y='energy', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Implement a vectorized version of the ReLU activation function $z \\mapsto \\max \\{0, z\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Open activation.py and implement the function ReLu\n",
    "\n",
    "from mlis.neuralnetwoks.activation import ReLu\n",
    "\n",
    "z = np.array([-.1, 0, 1.23, -1])\n",
    "testing.assert_array_almost_equal(ReLu(z), [0, 0, 1.23, 0], decimal=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def init(layer_sizes, scale, rng=np.random.RandomState(0)):\n",
    "    \"\"\"Build a list of randomly initialized (U, c) tuples, one for each layer.\"\"\"\n",
    "    params = []\n",
    "    for insize, outsize in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "        U = rng.randn(insize, outsize) * scale\n",
    "        c = rng.randn(outsize) * scale\n",
    "        params.append((U, c))\n",
    "    return params\n",
    "\n",
    "\n",
    "params = init(layer_sizes=[1, 4, 1], scale=0.5)\n",
    "\n",
    "for i, (U, c) in enumerate(params):\n",
    "    print(f'U_{i}: {U}, c_{i}: {c}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Implement the $k$-layer hypothesis function\n",
    "\n",
    "\\begin{align*}\n",
    "z_1 &= x\\\\\n",
    "z_{i+1} &= \\sigma(U_i z_i + c_i), \\quad i=1, \\dotsc, k-1\\\\\n",
    "h(x) &= U_k z_k + c_k\n",
    "\\end{align*}\n",
    "\n",
    "where $\\sigma$ is the activation function and the list `params` contains the tuples $(U_i, c_i)$ i.e. `[ (U_1, c_1), ..., (U_k, c_k) ]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Open feedforward.py and implement the function predict\n",
    "from mlis.neuralnetwoks.feedforward import predict\n",
    "\n",
    "# test the correctness of the implementation\n",
    "X = np.array([1, 2, 3])\n",
    "params = init(layer_sizes=[1, 4, 1], scale=0.5, rng=np.random.RandomState(0))\n",
    "\n",
    "for i, (U, c) in enumerate(params):\n",
    "    print(f'U_{i}: {U}, c_{i}: {c}')\n",
    "\n",
    "actual = predict(X, params)\n",
    "expected = np.array([[1.12], [1.92], [2.75]])\n",
    "testing.assert_array_almost_equal(actual, expected, decimal=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# fit scaler\n",
    "scaler = preprocessing.StandardScaler().fit(asinput(df['temp']))\n",
    "# load data\n",
    "X = scaler.transform(asinput(df['temp']))\n",
    "y = aslabel(df['energy'])\n",
    "\n",
    "# Here is our initial guess:\n",
    "params = init(scale=0.5, layer_sizes=[1, 4, 1], rng=np.random.RandomState(0))\n",
    "\n",
    "\n",
    "# Objective function\n",
    "def J(params, _):\n",
    "    pred = predict(X, params)\n",
    "    err = aslabel(y).reshape((-1, 1)) - pred\n",
    "    return np.mean(err ** 2)\n",
    "\n",
    "\n",
    "# print information during optimization\n",
    "def callback(params, step, g):\n",
    "    if step % 250 == 0:\n",
    "        print(f\"J(w): {J(params, step):1.5e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# good values are:\n",
    "# num_iters = 200\n",
    "# epochs = 20\n",
    "# step_size in [0.1, 0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# try different numbers iterations, epochs and step_sizes until the network learned a good approximation\n",
    "num_iters = 1\n",
    "epochs = 1\n",
    "\n",
    "for step_size in [1]:\n",
    "    print(f'step size: {step_size}')\n",
    "    for epoch in range(epochs):\n",
    "        print(f'\\tepoch: {epoch}', end='\\t')\n",
    "        params = adam(grad(J), params, step_size=step_size, num_iters=num_iters, callback=callback)\n",
    "        if J(params, None) < 2e-5:\n",
    "            break\n",
    "\n",
    "for i, (U, c) in enumerate(params):\n",
    "    print(f'U_{i}: {U}, c_{i}: {c}')\n",
    "\n",
    "t = np.linspace(df['temp'].min(), df['temp'].max())\n",
    "\n",
    "# plot data\n",
    "fig, ax = plt.subplots()\n",
    "sns.scatterplot(ax=ax, x='temp', y='energy', data=df)\n",
    "ax.plot(t, predict(scaler.transform(asinput(t)), params), 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# try different numbers iterations, epochs and step_sizes until the network learned a good approximation\n",
    "num_iters = 1\n",
    "epochs = 1\n",
    "\n",
    "for step_size in [1]:\n",
    "    print(f'step size: {step_size}')\n",
    "    for epoch in range(epochs):\n",
    "        print(f'\\tepoch: {epoch}', end='\\t')\n",
    "        params = adam(grad(J), params, step_size=step_size, num_iters=num_iters, callback=callback)\n",
    "        if J(params, None) < 2e-5:\n",
    "            break\n",
    "\n",
    "for i, (U, c) in enumerate(params):\n",
    "    print(f'U_{i}: {U}, c_{i}: {c}')\n",
    "\n",
    "t = np.linspace(df['temp'].min(), df['temp'].max())\n",
    "\n",
    "# plot data\n",
    "fig, ax = plt.subplots()\n",
    "sns.scatterplot(ax=ax, x='temp', y='energy', data=df)\n",
    "ax.plot(t, predict(scaler.transform(asinput(t)), params), 'r')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
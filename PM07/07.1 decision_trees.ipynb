{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Regression Trees\n",
    "\n",
    "revision: dcfbda7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# @formatter:off\n",
    "# PREAMBLE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from mlis.arrays import asinput, aslabel\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_context(\"notebook\", font_scale=1.1)\n",
    "sns.set_style(\"ticks\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# @formatter:on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To make things easier, we will only consider 1-dimensional inputs, that is $x\\in\\mathbb{R}$.\n",
    "The decision tree is implemented in the following order:\n",
    "1. Create a function that identifies the correct partition.\n",
    "2. Create a function that can make predictions, given a partition and the desired prediction for that partition.\n",
    "3. Create a function which greedily selects the next split.\n",
    "4. Create a function that iteratively fits a decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We will model the partitions as a list of split points `splits`. For example `splits = [20, 60]` defines 3 partitions $(-\\infty, 20]$, $(20, 60]$ and $(60, \\infty)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Open regression_tree.py and implement the function findSplitIdx\n",
    "\n",
    "from mlis.trees.regression_tree import findSplitIdx\n",
    "\n",
    "X = [-np.inf, 20, 20.01, 60, np.inf, 19.999]\n",
    "splits = [20.0, 60]\n",
    "np.testing.assert_array_equal(findSplitIdx(X, splits), [0, 0, 1, 1, 2, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Implement a function `predict` which returns `yhat[p]` if the input is in the partition with index `p`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Open regression_tree.py and implement the function predict\n",
    "\n",
    "from mlis.trees.regression_tree import predict\n",
    "\n",
    "X = [-np.inf, 20, 20.01, 60, np.inf, 19.999]\n",
    "splits = [20.0, 60]\n",
    "yhat = [10., 20., 30.]\n",
    "np.testing.assert_array_almost_equal(predict(X, yhat, splits), [10, 10, 20, 20, 30, 10], decimal=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Given training data $X$ and $y$, find $\\hat{y}$ for each partition if the squared loss is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Open regression_tree.py and implement the function fit_sq_loss\n",
    "\n",
    "from mlis.trees.regression_tree import fit_sq_loss\n",
    "\n",
    "X = [-np.inf, 20, 20.01, 60, np.inf, 19.999]\n",
    "y = [11, 12, 21, 22, 30, 13]\n",
    "splits = [20.0, 60]\n",
    "np.testing.assert_array_almost_equal(fit_sq_loss(X, y, splits), [12., 21.5, 30.], decimal=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Implement the objective function $J$ which tells us how good the split is using the square loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Open regression_tree.py and implement the function J\n",
    "\n",
    "from mlis.trees.regression_tree import J\n",
    "\n",
    "X = [-np.inf, 20, 20.01, 60, np.inf, 19.999]\n",
    "y = [11, 12, 21, 22, 30, 13]\n",
    "splits = [20.0, 60]\n",
    "np.testing.assert_array_almost_equal(J(X, y, splits), 0.417, decimal=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The only thing left is to find the `splits`.\n",
    "Given the current partition defined by `splits` find the next best split point.\n",
    "We define the best split to be in de *middle*\n",
    "If no split reduces the objective $J$, return `None`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Open regression_tree.py and implement the function find_next_split\n",
    "\n",
    "from mlis.trees.regression_tree import find_next_split\n",
    "\n",
    "X = [6, 1, 3, 3, 2, 4, 5]\n",
    "y = [3.01, 1, 1, 1, 1, 2, 3]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(X, y, 'x')\n",
    "\n",
    "np.testing.assert_array_almost_equal(find_next_split(X, y, []), 3.5, decimal=3)\n",
    "np.testing.assert_array_almost_equal(find_next_split(X, y, [3]), 4.5, decimal=3)\n",
    "np.testing.assert_array_almost_equal(find_next_split(X, y, [1.5, 2.5, 3.5, 4.5]), 5.5, decimal=3)\n",
    "assert find_next_split(X, y, [1.5, 2.5, 3.5, 4.5, 5.5]) is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Open regression_tree.py and implement the function fit_tree\n",
    "\n",
    "from mlis.trees.regression_tree import fit_tree\n",
    "\n",
    "X = [6, 1, 3, 3, 2, 4, 5]\n",
    "y = [3.01, 1, 1, 1, 1, 2, 3]\n",
    "t = [-1, 3.4, 3.6, 9]\n",
    "\n",
    "h = fit_tree(X, y, max_splits=1)\n",
    "np.testing.assert_array_almost_equal(h(t), [1., 1., 2.67, 2.67], decimal=3)\n",
    "h = fit_tree(X, y, max_splits=10)\n",
    "np.testing.assert_array_almost_equal(h(t), [1., 1., 2., 3.01], decimal=3)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(X, y, 'x')\n",
    "ax.plot(t, h(t), drawstyle='steps-pre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Test on real data. Reproduce figure 8.5 on page 73."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# test on real data\n",
    "# load the data\n",
    "df = pd.read_csv('energy.csv', sep=' ')\n",
    "X = asinput(df['temp'])\n",
    "y = aslabel(df['energy'])\n",
    "t = np.linspace(X.min(), X.max())\n",
    "\n",
    "for max_splits in [1, 3, 20, 100]:\n",
    "    h = fit_tree(X, y, max_splits)\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.scatterplot(ax=ax, x='temp', y='energy', data=df)\n",
    "    ax.plot(t, h(t), 'r', drawstyle='steps-pre')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}